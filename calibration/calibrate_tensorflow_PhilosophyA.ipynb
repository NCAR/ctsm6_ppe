{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51b1903-800e-4a03-8ab2-a014c4a01671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 05:27:09.889826: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-23 05:27:09.900305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750678029.909616   69481 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750678029.912521   69481 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-23 05:27:09.925094: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('/glade/u/home/linnia/ctsm6_ppe/')\n",
    "from utils.pyfunctions import *\n",
    "utils_path = '/glade/u/home/linnia/ctsm6_ppe/utils/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2742153f-0ed1-40f3-89b0-d389ec4655f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "ncores=1\n",
    "nmem='20GB'\n",
    "cluster = PBSCluster(\n",
    "    cores=ncores, # The number of cores you want\n",
    "    memory=nmem, # Amount of memory\n",
    "    processes=1, # How many processes\n",
    "    queue='casper', # The type of queue to utilize (/glade/u/apps/dav/opt/usr/bin/execcasper)\n",
    "    local_directory='$TMPDIR', # Use your local directory\n",
    "    resource_spec='select=1:ncpus='+str(ncores)+':mem='+nmem, # Specify resources\n",
    "    account='P93300041', # Input your project ID here\n",
    "    walltime='16:00:00', # Amount of wall time\n",
    "    #interface='ib0', # Interface to use\n",
    ")\n",
    "\n",
    "# Scale up\n",
    "cluster.scale(25)\n",
    "\n",
    "# Setup your client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75933f-caed-4066-97a0-9d0aae2c04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0cfac-a2b8-4e16-bea0-9c6bfa308372",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1283e30f-bb8c-494f-959b-7db9c849985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biome_param_names(b, u_params, pft_params):\n",
    "    \n",
    "    with open(utils_path+\"/biome_configs.pkl\", \"rb\") as f:\n",
    "        biome_configs = pickle.load(f)\n",
    "\n",
    "    param_names = list(u_params)\n",
    "    for pft in biome_configs[b]['pfts']:\n",
    "        pft_param_names = [f\"{param}_{pft}\" for param in pft_params]\n",
    "        param_names.extend(pft_param_names)\n",
    "\n",
    "    return param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4b0772-b71f-4a01-a407-4278d20d250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 05:27:19.514975: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "#======================== set up ============================\n",
    "# get parameter information\n",
    "with open(utils_dir+\"/param_names.pkl\", \"rb\") as f:\n",
    "    param_info = pickle.load(f)\n",
    "u_params = param_info['u_params']\n",
    "pft_params = param_info['pft_params']\n",
    "\n",
    "# get biome information\n",
    "with open(utils_dir+\"/biome_configs.pkl\", \"rb\") as f:\n",
    "    biome_configs = pickle.load(f)\n",
    "\n",
    "# get default parameter set and reset some settings of default parameters\n",
    "default_params = pd.read_csv('default_params_norm.csv', index_col=False)\n",
    "default_params.loc[0, ['jmaxb1']] = [0.4]\n",
    "default_params.loc[0, ['theta_cj']] = [0.7]\n",
    "default_params.loc[0, ['upplim_destruct_metamorph']] = [1]\n",
    "default_params.loc[0, ['xl_12']] = [0]\n",
    "\n",
    "# Build universal_set tensor\n",
    "universal_set = tf.constant(default_params[u_params].iloc[[0]].to_numpy(),dtype=tf.float64)\n",
    "\n",
    "x_default = tf.constant(default_params.drop(columns=u_params).iloc[[0]].to_numpy(), dtype=tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d453fff0-62c2-4142-8ddd-83e352eb7281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get observations \n",
    "obs_biome = xr.open_dataset('calibration_obsStatistics_sudokuBiomes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e3440bb-7778-48e2-b380-e5ee90175ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'lai': '/glade/u/home/linnia/ctsm6_ppe/calibration/emulators_biomelai_compiled/',\n",
    "    'gpp': '/glade/u/home/linnia/ctsm6_ppe/calibration/emulators_biomegpp_compiled/',\n",
    "    'biomass': '/glade/u/home/linnia/ctsm6_ppe/calibration/emulators_biomebiomass_compiled/',\n",
    "    'lh': '/glade/u/home/linnia/ctsm6_ppe/calibration/emulators_biomeet_compiled/',\n",
    "    'sh': '/glade/u/home/linnia/ctsm6_ppe/calibration/emulators_biomesh_compiled/'\n",
    "}\n",
    "\n",
    "biomes = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "param_indices = []\n",
    "targets = []\n",
    "stdevs = []\n",
    "emulators = []  \n",
    "\n",
    "for b in biomes:\n",
    "    biome_name = biome_configs[b]['name']\n",
    "    param_names = get_biome_param_names(b,u_params,pft_params)\n",
    "    param_ix = [default_params.columns.get_loc(p)for p in param_names]\n",
    "    \n",
    "    param_indices.append(param_ix)\n",
    "    param_indices.append(param_ix)\n",
    "    param_indices.append(param_ix)\n",
    "    param_indices.append(param_ix)\n",
    "    param_indices.append(param_ix)\n",
    "    \n",
    "    targets.extend([\n",
    "        tf.convert_to_tensor(1.3*obs_biome.LAI_mean.isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor(1.3*obs_biome.GPP_mean.isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor(1.3*obs_biome.TVC_mean.isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor(obs_biome.LE_mean.isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor(obs_biome.SH_mean.isel(biome=b).values, dtype=tf.float64)\n",
    "    ])\n",
    "    stdevs.extend([\n",
    "        tf.convert_to_tensor(obs_biome.LAI_stdev.isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor(obs_biome.GPP_stdev.isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor(obs_biome.TVC_stdev.isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor(obs_biome.LE_stdev.isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor(obs_biome.SH_stdev.isel(biome=b).values, dtype=tf.float64),\n",
    "    ])\n",
    "\n",
    "    emulators.extend([\n",
    "        tf.saved_model.load(f\"{paths['lai']}{biome_name}\"),\n",
    "        tf.saved_model.load(f\"{paths['gpp']}{biome_name}\"),\n",
    "        tf.saved_model.load(f\"{paths['biomass']}{biome_name}\"),\n",
    "        tf.saved_model.load(f\"{paths['lh']}{biome_name}\"),\n",
    "        tf.saved_model.load(f\"{paths['sh']}{biome_name}\")\n",
    "    ])\n",
    "    \n",
    "targets = tf.stack(targets, axis=0)  \n",
    "stdevs = tf.stack(stdevs, axis=0)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d04b8b79-560a-4d12-8cea-35c09f726d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— for fast graph execution ———\n",
    "lengths = [len(arr) for arr in param_indices]     # list of Python ints\n",
    "\n",
    "emulator_array = []\n",
    "for m,input_dim in zip(emulators, lengths):\n",
    "    sig = tf.TensorSpec([None, input_dim], tf.float64)\n",
    "    f = tf.function(lambda X, _m=m: _m.compiled_predict_f(X)[0], input_signature=[sig])\n",
    "    emulator_array.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a9247-0400-4c50-9fbe-b65d33348783",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel(time=slice('1985','2023'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b2565-3cf0-4eef-bbc0-1ccfbce748fa",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864b485d-eb40-466a-bd93-7d6c90f420fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def optimization_step_batch(x, universal_set, emulator_array, param_indices, optimizer, x_default, barrier_strength=1, lambda_penalty=0.01):\n",
    "    with tf.GradientTape() as tape:\n",
    "        batch = tf.shape(x)[0]\n",
    "        u_tiled = tf.tile(universal_set, [batch, 1])   \n",
    "        x_full = tf.concat([u_tiled, x], axis=1)\n",
    "        per_model_losses = []\n",
    "        \n",
    "        # loop over metrics and biomes in stacked 1D arrays. \n",
    "        for i in range(len(emulator_array)):\n",
    "            model   = emulator_array[i]\n",
    "\n",
    "            target = tf.reshape(targets[i], (1, -1))\n",
    "            stdev = tf.reshape(stdevs[i], (1, -1))\n",
    "            target_tiled = tf.tile(target, [batch, 1])\n",
    "            stdev_tiled = tf.tile(stdev, [batch, 1])\n",
    "        \n",
    "            ix  = param_indices[i]          \n",
    "            x_biome = tf.gather(x_full, ix, axis=1)\n",
    "            y_pred = model(x_biome)\n",
    "            z = tf.abs((y_pred - target_tiled)/stdev_tiled)\n",
    "\n",
    "            if epsilon is not None: # if biome/variable zscore is < epsilon, it no longer contributes to loss\n",
    "                mask = tf.cast(z > epsilon, tf.float64)\n",
    "                loss_b = tf.reduce_sum(mask * tf.square(z), axis=1)\n",
    "            else:\n",
    "                loss_b = tf.reduce_sum(tf.square(z), axis=1)\n",
    "            \n",
    "            # size [batch]\n",
    "            per_model_losses.append(loss_b)\n",
    "            \n",
    "        loss_per_sample = tf.add_n(per_model_losses)\n",
    "        data_loss = tf.reduce_mean(loss_per_sample) # overall scalar loss = mean of the batch\n",
    "        max_z = tf.sqrt(tf.reduce_max(loss_per_sample)) # worst individual biome/init zscore\n",
    "\n",
    "        # Penalty for moving from default\n",
    "        # sum over the parameter‐axis (axis=1) → shape [batch]\n",
    "        x_default_tiled = tf.tile(x_default, [batch, 1])\n",
    "        sum_abs = tf.reduce_sum(tf.abs(x - x_default_tiled), axis=1)\n",
    "        penalty = tf.reduce_mean(sum_abs) # mean of batch\n",
    "\n",
    "        ratio = penalty / lambda_penalty\n",
    "        factor = tf.maximum(ratio, 1.0) # if the penalty ==lambda, it no longer impacts loss\n",
    "        total_loss = data_loss * factor\n",
    "        \n",
    "        # penalty for being close to/outside of bounds (0,1)\n",
    "        barrier = tf.reduce_mean(1.0 / (x + 1e-6) + 1.0 / (1.0 - x + 1e-6))\n",
    "        total_loss = total_loss * (1.0 + barrier_strength * barrier)\n",
    "        \n",
    "    grads = tape.gradient(total_loss, [x])\n",
    "    optimizer.apply_gradients(zip(grads, [x]))\n",
    "    x.assign(tf.clip_by_value(x, 0.0, 1.0))\n",
    "    return total_loss, max_z, data_loss, factor, barrier_strength*barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2522e4e-80d8-41fa-82e6-d9dd88812ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_optimization(x, universal_set, emulator_array, param_indices, x_default, maxiter, lr, lr_decay_steps, checkpoint_N, checkpoint_dir, epsilon, barrier_strength, lambda_penalty=0.0):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr,\n",
    "        decay_steps=lr_decay_steps,\n",
    "        decay_rate=0.5,\n",
    "        staircase=True)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    history_total_loss = []\n",
    "    history_max_z = []\n",
    "    history_data_loss = []\n",
    "    history_penalty = []\n",
    "    history_barrier = []\n",
    "    \n",
    "    for step in range(maxiter):\n",
    "\n",
    "        total_loss, max_z, data_loss, penalty, barrier_loss = optimization_step_batch(x, universal_set, emulator_array, param_indices, optimizer, x_default, barrier_strength, lambda_penalty)\n",
    "\n",
    "        # log\n",
    "        history_total_loss.append(total_loss.numpy())\n",
    "        history_max_z.append(max_z.numpy())\n",
    "        history_data_loss.append(data_loss.numpy())\n",
    "        history_penalty.append(penalty.numpy())\n",
    "        history_barrier.append(barrier_loss.numpy())\n",
    "\n",
    "        # print updates to screen\n",
    "        if step % 10 == 0:\n",
    "            tf.print(f\"Step {step:03d}: total={total_loss:.6f} max_z={max_z:.6f}\")\n",
    "    \n",
    "        # Save a checkpoint every N iterations\n",
    "        if step % checkpoint_N == 0:\n",
    "            checkpoint = {\n",
    "                'step': step,\n",
    "                'params': x,\n",
    "                'loss': total_loss,\n",
    "            }\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_step_{step}.pkl')\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pickle.dump(checkpoint, f)\n",
    "                \n",
    "        # early stopping\n",
    "        if tf.reduce_max(max_z) <= epsilon:\n",
    "            print(f\"Converged at step {step}\")\n",
    "            tf.print(f\"Step {step:03d}: total={total_loss:.6f} max_z={max_z:.6f}\")\n",
    "            break\n",
    "    \n",
    "    x_opt = x.numpy()\n",
    "    logs = {\n",
    "        'total_loss': history_total_loss,\n",
    "        'max_z': history_max_z,\n",
    "        'data_loss': history_data_loss,\n",
    "        'penalty': history_penalty,\n",
    "        'barrier': history_barrier\n",
    "    }\n",
    "    \n",
    "    return x_opt, logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae99e81-ab68-45ac-b878-46a3c3ca9a3e",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9fbd825-43b9-464d-b873-52a8d8b818cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization: sample from normal priors around default\n",
    "pft_cols     = [c for c in default_params.columns if c not in u_params]\n",
    "default_pftparams = default_params[pft_cols].iloc[0].values   \n",
    "\n",
    "n_inits = 1000\n",
    "sigma   = 0.1   # spread of your normal draws\n",
    "\n",
    "np.random.seed(42)   \n",
    "x0 = np.random.normal(loc=default_pftparams,\n",
    "                      scale=sigma,\n",
    "                      size=(n_inits, default_pftparams.size))\n",
    "\n",
    "# enforce bounds [0,1]\n",
    "x0 = np.clip(x0, 0.0, 1.0)\n",
    "\n",
    "x_init = tf.Variable(\n",
    "    tf.constant(x0, dtype=tf.float64),\n",
    "    trainable=True,\n",
    "    name=\"x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1d53c5b-7235-4ab5-967f-ad9b1bf4de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== save ======================================\n",
    "n_inits = np.shape(x0)[0]\n",
    "uset_tiled = np.tile(default_params[u_params].values[0], (n_inits,1))\n",
    "init_sets = np.concatenate([uset_tiled,x0],axis=1)\n",
    "init_paramsets = pd.DataFrame(init_sets,columns=default_params.columns.values)\n",
    "\n",
    "outfile = 'init_sets_defaultPrior.csv'\n",
    "init_paramsets.to_csv(outfile,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13879efc-de9e-4200-a34e-bc130948b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or initialize from a previous optimization\n",
    "with open(\"./checkpoints_close2default_LHSH/checkpoint_step_500.pkl\", \"rb\") as f:\n",
    "    c0 = pickle.load(f)\n",
    "\n",
    "x_init = tf.Variable(\n",
    "    tf.constant(c0['params'], dtype=tf.float64),\n",
    "    trainable=True,\n",
    "    name=\"x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c29b3eb-23a2-41e5-abdb-bf321e0011d7",
   "metadata": {},
   "source": [
    "### calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cec55-2e32-4f14-9e83-9dc56f072241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 000: total=504.486239 max_z=28.177227\n",
      "Step 010: total=244.585974 max_z=18.441436\n",
      "Step 020: total=191.300785 max_z=15.079896\n",
      "Step 030: total=156.966256 max_z=13.783480\n",
      "Step 040: total=139.843852 max_z=12.666163\n",
      "Step 050: total=127.397916 max_z=12.115661\n",
      "Step 060: total=119.080536 max_z=11.614920\n",
      "Step 070: total=113.205697 max_z=11.274842\n",
      "Step 080: total=109.157926 max_z=10.975626\n",
      "Step 090: total=105.974060 max_z=10.761264\n",
      "Step 100: total=103.707094 max_z=10.588688\n",
      "Step 110: total=101.755886 max_z=10.435859\n",
      "Step 120: total=100.389407 max_z=10.363657\n",
      "Step 130: total=99.191542 max_z=10.305518\n",
      "Step 140: total=98.093750 max_z=10.229978\n",
      "Step 150: total=97.045332 max_z=10.179100\n",
      "Step 160: total=96.244273 max_z=10.106254\n",
      "Step 170: total=95.481469 max_z=10.089331\n",
      "Step 180: total=94.900901 max_z=10.076341\n",
      "Step 190: total=94.149933 max_z=10.021079\n",
      "Step 200: total=93.639813 max_z=10.002640\n",
      "Step 210: total=93.184816 max_z=9.975901\n",
      "Step 220: total=92.577959 max_z=9.956634\n",
      "Step 230: total=92.317511 max_z=9.918309\n",
      "Step 240: total=91.882417 max_z=9.913049\n",
      "Step 250: total=91.512425 max_z=9.938623\n",
      "Step 260: total=91.154222 max_z=9.887334\n",
      "Step 270: total=90.806502 max_z=9.868236\n",
      "Step 280: total=90.678947 max_z=9.858465\n",
      "Step 290: total=90.300054 max_z=9.837442\n",
      "Step 300: total=90.217431 max_z=9.843033\n",
      "Step 310: total=89.860315 max_z=9.844572\n",
      "Step 320: total=89.695587 max_z=9.823263\n",
      "Step 330: total=89.705696 max_z=9.804666\n",
      "Step 340: total=89.596685 max_z=9.810456\n",
      "Step 350: total=89.495097 max_z=9.777549\n",
      "Step 360: total=89.452103 max_z=9.783897\n",
      "Step 370: total=89.315370 max_z=9.763431\n",
      "Step 380: total=89.246182 max_z=9.761374\n",
      "Step 390: total=89.158794 max_z=9.753366\n",
      "Step 400: total=89.045631 max_z=9.774102\n",
      "Step 410: total=89.138277 max_z=9.758858\n",
      "Step 420: total=89.044499 max_z=9.739409\n",
      "Step 430: total=88.950032 max_z=9.736216\n",
      "Step 440: total=88.927741 max_z=9.708752\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run optimization\n",
    "epsilon = 0.5\n",
    "lr = 1e-2\n",
    "maxiter=2000\n",
    "lr_decay_steps = 300\n",
    "checkpoint_N = 50\n",
    "checkpoint_dir = './checkpoints_plus30percent/'\n",
    "lambda_penalty = 30 # worst sum of delta from default you will tolerate\n",
    "barrier_strength = 0 # clipping is on\n",
    "\n",
    "x_opt, logs = run_optimization(x_init, universal_set, emulator_array, param_indices, x_default, \n",
    "                               maxiter, lr, lr_decay_steps, checkpoint_N, checkpoint_dir, epsilon, barrier_strength, lambda_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9e001-8121-4ffa-82ea-90a19e65d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== save ======================================\n",
    "n_inits = np.shape(x_opt)[0]\n",
    "uset_tiled = np.tile(default_params[u_params].values[0], (n_inits,1))\n",
    "opt_sets = np.concatenate([uset_tiled,x_opt],axis=1)\n",
    "calibrated_paramsets = pd.DataFrame(opt_sets,columns=default_params.columns.values)\n",
    "\n",
    "outfile = checkpoint_dir+'calibrated_sets_plus30percent_lambda30_052625.csv'\n",
    "calibrated_paramsets.to_csv(outfile,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b623af76-b672-460b-a11b-4b2d2836be5f",
   "metadata": {},
   "source": [
    "### plot convergence logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4c67b-5b7d-4576-83ad-b097a9ee71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(logs['total_loss'], label='Total loss')\n",
    "plt.plot(logs['data_loss'], label='Data loss')\n",
    "plt.plot(logs['penalty'], label='Penalty loss')\n",
    "plt.plot(logs['barrier'], label='Barrier loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Loss components over optimization steps')\n",
    "plt.ylim(0,30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297815de-2a89-4397-8816-10145f2b9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tiled = np.tile(default_params.drop(columns=u_params).values[0], (n_inits,1))\n",
    "np.sum(np.abs(x_opt - d_tiled),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d835724-7780-416f-91eb-ea681987dc36",
   "metadata": {},
   "source": [
    "### Plot posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86b6a9-b51e-454c-a521-1999a152fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_calibrated_paramsets = pd.read_csv('calibrated_sets_unifRandom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a6c08d-e86a-4b31-a949-95a216fb50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LEAP colors\n",
    "leap_colors = ['#B9D9EB', '#00796B', '#012169']\n",
    "\n",
    "# Prepare data\n",
    "pft = 4\n",
    "pft_param_names = [f\"{param}_{pft}\" for param in pft_params]\n",
    "data = full_calibrated_paramsets[pft_param_names]\n",
    "\n",
    "n = len(pft_param_names)\n",
    "\n",
    "# Set plot-wide styles\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,         # Font size\n",
    "    'axes.labelsize': 12,    # Axis label size\n",
    "    'xtick.labelsize': 8, \n",
    "    'ytick.labelsize': 8,\n",
    "    'axes.linewidth': 0.8,   # Thin frame lines\n",
    "    'savefig.dpi': 300,\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(n, n, figsize=(2*n, 2*n))  # Larger, scalable size\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        ax = axes[i, j]\n",
    "        if i == j:\n",
    "            # 1D histogram on the diagonal\n",
    "            ax.hist(data[pft_param_names[i]], bins=50, color=leap_colors[2], alpha=0.7)\n",
    "            ax.axvline(default_params[pft_param_names[i]].values)\n",
    "            ax.set_xlim([0,1])\n",
    "        else:\n",
    "            # 2D hexbin on the off-diagonals\n",
    "            hb = ax.hexbin(\n",
    "                data[pft_param_names[j]],\n",
    "                data[pft_param_names[i]],\n",
    "                gridsize=50,\n",
    "                #cmap='Blues',\n",
    "                vmin=0,\n",
    "                vmax=20,\n",
    "                mincnt=1,           # Only show bins with data\n",
    "                linewidths=0\n",
    "            )\n",
    "            # Fix hexbin color to LEAP navy color\n",
    "            #hb.set_cmap('Blues')\n",
    "            #hb.set_array(hb.get_array()*0+1)  # Uniform color intensity\n",
    "            #hb.set_edgecolor('none')\n",
    "            #hb.set_facecolor(leap_colors[2])  # Dark navy\n",
    "            ax.axvline(default_params[pft_param_names[j]].values)\n",
    "            ax.axhline(default_params[pft_param_names[i]].values)\n",
    "            #ax.set_ylim([0,1])\n",
    "            #ax.set_xlim([0,1])\n",
    "\n",
    "        # Clean ticks\n",
    "        if i < n-1:\n",
    "            ax.set_xticklabels([])\n",
    "        else:\n",
    "            ax.set_xlabel(pft_param_names[j], fontsize=10)\n",
    "        if j > 0:\n",
    "            ax.set_yticklabels([])\n",
    "        else:\n",
    "            ax.set_ylabel(pft_param_names[i], fontsize=10)\n",
    "\n",
    "plt.tight_layout(pad=0.5)\n",
    "#plt.savefig('./figs/hexbin_scattermatrix_pftparams_pft'+str(pft)+'.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c087a6e-1fce-4a4f-bb1b-b285d0aa5fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlenv]",
   "language": "python",
   "name": "conda-env-mlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
