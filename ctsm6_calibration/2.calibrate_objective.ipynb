{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51b1903-800e-4a03-8ab2-a014c4a01671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 17:06:45.753955: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-26 17:06:45.756741: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-26 17:06:45.763687: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756249605.774320    7983 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756249605.777347    7983 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-26 17:06:45.789877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('/glade/u/home/linnia/ctsm6_ppe/')\n",
    "from utils.pyfunctions import *\n",
    "utils_path = '/glade/u/home/linnia/ctsm6_ppe/utils/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2742153f-0ed1-40f3-89b0-d389ec4655f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "ncores=1\n",
    "nmem='10GB'\n",
    "cluster = PBSCluster(\n",
    "    cores=ncores, # The number of cores you want\n",
    "    memory=nmem, # Amount of memory\n",
    "    processes=1, # How many processes\n",
    "    queue='casper', # The type of queue to utilize (/glade/u/apps/dav/opt/usr/bin/execcasper)\n",
    "    local_directory='$TMPDIR', # Use your local directory\n",
    "    resource_spec='select=1:ncpus='+str(ncores)+':mem='+nmem, # Specify resources\n",
    "    account='P93300041', # Input your project ID here\n",
    "    walltime='5:00:00', # Amount of wall time\n",
    "    #interface='ib0', # Interface to use\n",
    ")\n",
    "\n",
    "# Scale up\n",
    "cluster.scale(10)\n",
    "\n",
    "# Setup your client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0cfac-a2b8-4e16-bea0-9c6bfa308372",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1283e30f-bb8c-494f-959b-7db9c849985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biome_param_names(b, u_params, pft_params):\n",
    "    \n",
    "    with open(\"./biome_configs.pkl\", \"rb\") as f:\n",
    "        biome_configs = pickle.load(f)\n",
    "\n",
    "    param_names = list(u_params)\n",
    "    for pft in biome_configs[b]['pfts']:\n",
    "        pft_param_names = [f\"{param}_{pft}\" for param in pft_params]\n",
    "        param_names.extend(pft_param_names)\n",
    "\n",
    "    return param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4b0772-b71f-4a01-a407-4278d20d250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 14:40:57.868837: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "#======================== set up ============================\n",
    "# get parameter information\n",
    "with open(utils_dir+\"/param_names.pkl\", \"rb\") as f:\n",
    "    param_info = pickle.load(f)\n",
    "u_params = param_info['u_params']\n",
    "pft_params = param_info['pft_params']\n",
    "\n",
    "# get biome information\n",
    "with open(\"./biome_configs.pkl\", \"rb\") as f:\n",
    "    biome_configs = pickle.load(f)\n",
    "\n",
    "# get default parameter set and reset some settings of default parameters\n",
    "default_params = pd.read_csv('default_params_norm.csv', index_col=False)\n",
    "default_params.loc[0, ['jmaxb1']] = [0.4]\n",
    "default_params.loc[0, ['theta_cj']] = [0.7]\n",
    "default_params.loc[0, ['upplim_destruct_metamorph']] = [1]\n",
    "default_params.loc[0, ['xl_12']] = [0]\n",
    "default_params.loc[0, ['TAU']] = [0.7]\n",
    "default_params.loc[0, ['RF_SS']] = [0.3]\n",
    "\n",
    "default_params.loc[0,['leafcn_12']] = [0.9831]\n",
    "default_params.loc[0,['slatop_12']] = [0.0315]\n",
    "\n",
    "default_params.loc[0,['leafcn_11']] = [0.9984]\n",
    "default_params.loc[0,['slatop_11']] = [0.0315]\n",
    "\n",
    "default_params.loc[0,['KCN_14']] = [0]\n",
    "\n",
    "# Build universal_set tensor\n",
    "universal_set = tf.constant(default_params[u_params].iloc[[0]].to_numpy(),dtype=tf.float64)\n",
    "\n",
    "x_default = tf.constant(default_params.drop(columns=u_params).iloc[[0]].to_numpy(), dtype=tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453fff0-62c2-4142-8ddd-83e352eb7281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get observations\n",
    "obs_biome = xr.open_dataset('calibration_obsStatistics_sudokuBiomes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e3440bb-7778-48e2-b380-e5ee90175ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'lai': './emulators_biomelai_compiled/',\n",
    "    'gpp': './emulators_biomegpp_compiled/',\n",
    "    'biomass': './emulators_biomebiomass_compiled/',\n",
    "}\n",
    "\n",
    "biomes = [1,2,3,4,5,7,8,9,10,11,12]\n",
    "param_indices = []\n",
    "target = []\n",
    "stdev = []\n",
    "emulators = []  \n",
    "\n",
    "for b in biomes:\n",
    "    biome_name = biome_configs[b]['name']\n",
    "    param_names = get_biome_param_names(b,u_params,pft_params)\n",
    "    param_ix = [default_params.columns.get_loc(p)for p in param_names]\n",
    "    \n",
    "    param_indices.append(param_ix)\n",
    "    param_indices.append(param_ix)\n",
    "    param_indices.append(param_ix)\n",
    "    \n",
    "    target.extend([\n",
    "        tf.convert_to_tensor((obs_biome.LAI_mean).isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor((obs_biome.GPP_mean).isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor((obs_biome.TVC_mean).isel(biome=b).values, dtype=tf.float64)\n",
    "    ])\n",
    "    stdev.extend([\n",
    "        tf.convert_to_tensor((obs_biome.LAI_stdev).isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor((obs_biome.GPP_stdev).isel(biome=b).values, dtype=tf.float64),\n",
    "        tf.convert_to_tensor((obs_biome.TVC_stdev).isel(biome=b).values, dtype=tf.float64)\n",
    "    ])\n",
    "    emulators.extend([\n",
    "        tf.saved_model.load(f\"{paths['lai']}{biome_name}\"),\n",
    "        tf.saved_model.load(f\"{paths['gpp']}{biome_name}\"),\n",
    "        tf.saved_model.load(f\"{paths['biomass']}{biome_name}\")\n",
    "    ])\n",
    "    \n",
    "targets = tf.stack(target, axis=0)\n",
    "stdevs = tf.stack(stdev, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d04b8b79-560a-4d12-8cea-35c09f726d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— for fast graph execution ———\n",
    "lengths = [len(arr) for arr in param_indices]     # list of Python ints\n",
    "\n",
    "emulator_array = []\n",
    "for m,input_dim in zip(emulators, lengths):\n",
    "    sig = tf.TensorSpec([None, input_dim], tf.float64)\n",
    "    f = tf.function(lambda X, _m=m: _m.compiled_predict_f(X)[0], input_signature=[sig])\n",
    "    emulator_array.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32aca530-4b7d-4366-96a8-c3c232835ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for PFT we don't want to calibrate\n",
    "pft_cols = [c for c in default_params.columns if c not in u_params]\n",
    "default_pftparams = default_params[pft_cols]\n",
    "n_params = default_pftparams.shape[1]\n",
    "\n",
    "mask = np.ones(n_params, dtype=bool)\n",
    "\n",
    "pfts_dontcal = [10, 12, 14]\n",
    "for pft in pfts_dontcal:\n",
    "    pft_param_names = [f\"{param}_{pft}\" for param in pft_params]\n",
    "    pft_param_ix = [default_pftparams.columns.get_loc(p) for p in pft_param_names]\n",
    "    for ix in pft_param_ix:\n",
    "        mask[ix] = False\n",
    "\n",
    "trainable_mask_tf = tf.constant(mask, dtype=tf.bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b2565-3cf0-4eef-bbc0-1ccfbce748fa",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "864b485d-eb40-466a-bd93-7d6c90f420fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def optimization_step_batch(x, universal_set, emulator_array, param_indices, optimizer, x_default, barrier_strength=1, lambda_penalty=0.01, trainable_mask=None):\n",
    "    with tf.GradientTape() as tape:\n",
    "        batch = tf.shape(x)[0]\n",
    "        u_tiled = tf.tile(universal_set, [batch, 1])   \n",
    "        x_full = tf.concat([u_tiled, x], axis=1)\n",
    "        per_model_losses = []\n",
    "        \n",
    "        # loop over metrics and biomes in stacked 1D arrays. \n",
    "        for i in range(len(emulator_array)):\n",
    "            model   = emulator_array[i]\n",
    "\n",
    "            target = tf.reshape(targets[i], (1, -1))\n",
    "            stdev = tf.reshape(stdevs[i], (1, -1))\n",
    "            target_tiled = tf.tile(target, [batch, 1])\n",
    "            stdev_tiled = tf.tile(stdev, [batch, 1])\n",
    "        \n",
    "            ix  = param_indices[i]          \n",
    "            x_biome = tf.gather(x_full, ix, axis=1)\n",
    "            y_pred = model(x_biome)\n",
    "            z = tf.abs((y_pred - target_tiled)/stdev_tiled)\n",
    "\n",
    "            if epsilon is not None: # if biome/variable zscore is < epsilon, it no longer contributes to loss\n",
    "                mask = tf.cast(z > epsilon, tf.float64)\n",
    "                loss_b = tf.reduce_sum(mask * tf.square(z), axis=1)\n",
    "            else:\n",
    "                loss_b = tf.reduce_sum(tf.square(z), axis=1)\n",
    "            \n",
    "            # size [batch]\n",
    "            per_model_losses.append(loss_b)\n",
    "            \n",
    "        loss_per_sample = tf.add_n(per_model_losses)\n",
    "        data_loss = tf.reduce_mean(loss_per_sample) # overall scalar loss = mean of the batch\n",
    "        max_z = tf.sqrt(tf.reduce_max(loss_per_sample)) # worst individual biome/init zscore\n",
    "\n",
    "        # Penalty for moving from default\n",
    "        # sum over the parameter‐axis (axis=1) → shape [batch]\n",
    "        x_default_tiled = tf.tile(x_default, [batch, 1])\n",
    "        sum_abs = tf.reduce_sum(tf.abs(x - x_default_tiled), axis=1)\n",
    "        penalty = tf.reduce_mean(sum_abs) # mean of batch\n",
    "\n",
    "        ratio = penalty / lambda_penalty\n",
    "        factor = tf.maximum(ratio, 1.0) # if the penalty <=lambda, it no longer impacts loss\n",
    "        total_loss = data_loss * factor\n",
    "        \n",
    "        # penalty for being close to/outside of bounds (0,1)\n",
    "        barrier = tf.reduce_mean(1.0 / (x + 1e-6) + 1.0 / (1.0 - x + 1e-6))\n",
    "        total_loss = total_loss * (1.0 + barrier_strength * barrier)\n",
    "        \n",
    "    grads = tape.gradient(total_loss, [x])\n",
    "\n",
    "    if trainable_mask is not None: # mask grad of PFTs we don't want to train\n",
    "        mask_broadcast = tf.reshape(trainable_mask, (1, -1))\n",
    "        grads = [tf.where(mask_broadcast, grads, tf.zeros_like(grads[0]))[0]]\n",
    "\n",
    "    optimizer.apply_gradients(zip(grads, [x]))\n",
    "    x.assign(tf.clip_by_value(x, 0.0, 1.0))\n",
    "    return total_loss, max_z, data_loss, factor, barrier_strength*barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2522e4e-80d8-41fa-82e6-d9dd88812ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_optimization(x, universal_set, emulator_array, param_indices, x_default, maxiter, lr, lr_decay_steps, checkpoint_N, checkpoint_dir, epsilon, barrier_strength=0, lambda_penalty=0.0,trainable_mask=None):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr,\n",
    "        decay_steps=lr_decay_steps,\n",
    "        decay_rate=0.5,\n",
    "        staircase=True)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    history_total_loss = []\n",
    "    history_max_z = []\n",
    "    history_data_loss = []\n",
    "    history_penalty = []\n",
    "    history_barrier = []\n",
    "    \n",
    "    for step in range(maxiter):\n",
    "\n",
    "        total_loss, max_z, data_loss, penalty, barrier_loss = optimization_step_batch(x, universal_set, emulator_array, param_indices, optimizer, x_default, barrier_strength, lambda_penalty,trainable_mask)\n",
    "\n",
    "        # log\n",
    "        history_total_loss.append(total_loss.numpy())\n",
    "        history_max_z.append(max_z.numpy())\n",
    "        history_data_loss.append(data_loss.numpy())\n",
    "        history_penalty.append(penalty.numpy())\n",
    "        history_barrier.append(barrier_loss.numpy())\n",
    "\n",
    "        # print updates to screen\n",
    "        if step % 10 == 0:\n",
    "            tf.print(f\"Step {step:03d}: total={total_loss:.6f} max_z={max_z:.6f}\")\n",
    "    \n",
    "        # Save a checkpoint every N iterations\n",
    "        if step % checkpoint_N == 0:\n",
    "            checkpoint = {\n",
    "                'step': step,\n",
    "                'params': x,\n",
    "                'loss': total_loss,\n",
    "            }\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_step_{step}.pkl')\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                pickle.dump(checkpoint, f)\n",
    "                \n",
    "        # early stopping\n",
    "        if tf.reduce_max(max_z) <= epsilon:\n",
    "            print(f\"Converged at step {step}\")\n",
    "            tf.print(f\"Step {step:03d}: total={total_loss:.6f} max_z={max_z:.6f}\")\n",
    "            break\n",
    "    \n",
    "    x_opt = x.numpy()\n",
    "    logs = {\n",
    "        'total_loss': history_total_loss,\n",
    "        'max_z': history_max_z,\n",
    "        'data_loss': history_data_loss,\n",
    "        'penalty': history_penalty,\n",
    "        'barrier': history_barrier\n",
    "    }\n",
    "    \n",
    "    return x_opt, logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae99e81-ab68-45ac-b878-46a3c3ca9a3e",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9fbd825-43b9-464d-b873-52a8d8b818cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization: sample from normal priors around default\n",
    "pft_cols     = [c for c in default_params.columns if c not in u_params]\n",
    "default_pftparams = default_params[pft_cols].iloc[0].values   \n",
    "\n",
    "n_inits = 1000\n",
    "sigma   = 0.1   # spread of your normal draws\n",
    "\n",
    "np.random.seed(42)   \n",
    "x0 = np.random.normal(loc=default_pftparams,\n",
    "                      scale=sigma,\n",
    "                      size=(n_inits, default_pftparams.size))\n",
    "\n",
    "# enforce bounds [0,1]\n",
    "x0 = np.clip(x0, 0.0, 1.0)\n",
    "\n",
    "x_init = tf.Variable(\n",
    "    tf.constant(x0, dtype=tf.float64),\n",
    "    trainable=True,\n",
    "    name=\"x\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "515e12a6-6a2d-486b-a9cb-42585ed7beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pft in pfts_dontcal:\n",
    "    pft_param_names = [f\"{param}_{pft}\" for param in pft_params]\n",
    "    pft_cols     = [c for c in default_params.columns if c not in u_params]\n",
    "    default_pftparams = default_params[pft_cols]\n",
    "    pft_param_ix = [default_pftparams.columns.get_loc(p)for p in pft_param_names]\n",
    "    x0[:,pft_param_ix] = np.tile(default_params[pft_param_names],(n_inits,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa5b58d-26b9-4279-aa3c-0e3be402c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = tf.Variable(\n",
    "    tf.constant(x0, dtype=tf.float64),\n",
    "    trainable=True,\n",
    "    name=\"x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c29b3eb-23a2-41e5-abdb-bf321e0011d7",
   "metadata": {},
   "source": [
    "### calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379cec55-2e32-4f14-9e83-9dc56f072241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run optimization\n",
    "epsilon = 1\n",
    "lr = 1e-2\n",
    "maxiter=1000\n",
    "lr_decay_steps = 100\n",
    "checkpoint_N = 50\n",
    "checkpoint_dir = './checkpoints_objective_v3/'\n",
    "lambda_penalty = 25 # worst sum of delta from default you will tolerate\n",
    "barrier_strength = 0 # clipping is on\n",
    "\n",
    "x_opt, logs = run_optimization(x_init, universal_set, emulator_array, param_indices, x_default, \n",
    "                               maxiter, lr, lr_decay_steps, checkpoint_N, checkpoint_dir, \n",
    "                               epsilon, barrier_strength, lambda_penalty,trainable_mask_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9e001-8121-4ffa-82ea-90a19e65d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== save ======================================\n",
    "n_inits = np.shape(x_opt)[0]\n",
    "uset_tiled = np.tile(default_params[u_params].values[0], (n_inits,1))\n",
    "opt_sets = np.concatenate([uset_tiled,x_opt],axis=1)\n",
    "calibrated_paramsets = pd.DataFrame(opt_sets,columns=default_params.columns.values)\n",
    "\n",
    "outfile = checkpoint_dir+'calibrated_sets_lambda25_073125.csv'\n",
    "calibrated_paramsets.to_csv(outfile,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b623af76-b672-460b-a11b-4b2d2836be5f",
   "metadata": {},
   "source": [
    "### plot convergence logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4c67b-5b7d-4576-83ad-b097a9ee71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(logs['total_loss'], label='Total loss')\n",
    "plt.plot(logs['data_loss'], label='Data loss')\n",
    "plt.plot(logs['penalty'], linestyle='--',label='Penalty loss')\n",
    "#plt.plot(logs['barrier'], label='Barrier loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Loss components over optimization steps')\n",
    "plt.ylim(0,100)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
